{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c317cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer , sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6fc84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_excel('../Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9358983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-telehealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telemedici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-people-di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/is-telehealth...\n",
       "1       2  https://insights.blackcoffer.com/how-telehealt...\n",
       "2       3  https://insights.blackcoffer.com/is-telemedici...\n",
       "3       4  https://insights.blackcoffer.com/is-telehealth...\n",
       "4       5  https://insights.blackcoffer.com/how-people-di..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a25c529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = input_df['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41cd2831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://insights.blackcoffer.com/is-telehealth...\n",
       "1      https://insights.blackcoffer.com/how-telehealt...\n",
       "2      https://insights.blackcoffer.com/is-telemedici...\n",
       "3      https://insights.blackcoffer.com/is-telehealth...\n",
       "4      https://insights.blackcoffer.com/how-people-di...\n",
       "                             ...                        \n",
       "145    https://insights.blackcoffer.com/blockchain-fo...\n",
       "146    https://insights.blackcoffer.com/the-future-of...\n",
       "147    https://insights.blackcoffer.com/big-data-anal...\n",
       "148    https://insights.blackcoffer.com/business-anal...\n",
       "149    https://insights.blackcoffer.com/challenges-an...\n",
       "Name: URL, Length: 150, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8fc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page=requests.get(URLs[0] , headers={\"User-Agent\": \"XY\"})\n",
    "# page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8f3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading positive words\n",
    "with open('../MasterDictionary/positive-words.txt','r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')\n",
    "\n",
    "# Loading negative words\n",
    "with open('../MasterDictionary/negative-words.txt','r' ,  encoding=\"ISO-8859-1\") as negfile:\n",
    "    negativeword=negfile.read().lower()\n",
    "negativeWordList=negativeword.split('\\n')\n",
    "\n",
    "#Loading stop words dictionary for removing stop words\n",
    "# GENERIC\n",
    "with open('../StopWords/StopWords_Generic.txt','r' ,  encoding=\"ISO-8859-1\") as genericfile:\n",
    "    genericword=genericfile.read().lower()\n",
    "GenericWordList=genericword.split('\\n')\n",
    "\n",
    "# GENERIC LONG\n",
    "with open('../StopWords/StopWords_GenericLong.txt','r' ,  encoding=\"ISO-8859-1\") as genericlongfile:\n",
    "    genericlongword=genericlongfile.read().lower()\n",
    "GenericLongWordList=genericlongword.split('\\n')\n",
    "\n",
    "# NAMES\n",
    "with open('../StopWords/StopWords_Names.txt','r' ,  encoding=\"ISO-8859-1\") as namefile:\n",
    "    names=namefile.read().lower()\n",
    "nameList=names.split('\\n')\n",
    "\n",
    "# GEOGRAPHIC\n",
    "with open('../StopWords/StopWords_Geographic.txt','r' ,  encoding=\"ISO-8859-1\") as geofile:\n",
    "    geographics=geofile.read().lower()\n",
    "geographicList=geographics.split('\\n')\n",
    "\n",
    "# GENERIC DATES\n",
    "with open('../StopWords/StopWords_DatesandNumbers.txt','r' ,  encoding=\"ISO-8859-1\") as datefile:\n",
    "    date=datefile.read().lower()\n",
    "dateList=date.split('\\n')\n",
    "\n",
    "# CURRENCIES\n",
    "with open('../StopWords/StopWords_Currencies.txt','r' ,  encoding=\"ISO-8859-1\") as currenciesfile:\n",
    "    currencies=currenciesfile.read().lower()\n",
    "currenciesList=currencies.split('\\n')\n",
    "\n",
    "# AUDITOR\n",
    "with open('../StopWords/StopWords_Auditor.txt','r' ,  encoding=\"ISO-8859-1\") as auditfile:\n",
    "    audit=auditfile.read().lower()\n",
    "auditList=audit.split('\\n')\n",
    "# with open(stopWordsFile ,'r') as stop_words:\n",
    "#     stopWords = stop_words.read().lower()\n",
    "# stopWordList = stopWords.split('\\n')\n",
    "# stopWordList[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322d223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in GenericWordList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in GenericLongWordList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in nameList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in geographicList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in dateList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in currenciesList, tokens))\n",
    "    filtered_words = list(filter(lambda token: token not in auditList, tokens))\n",
    "    return filtered_words\n",
    "\n",
    "def positive_score(text):\n",
    "    posword=0\n",
    "    tokenphrase = tokenizer(text)\n",
    "    for word in tokenphrase:\n",
    "        if word in positiveWordList:\n",
    "            posword+=1\n",
    "    retpos = posword\n",
    "    return retpos \n",
    "\n",
    "def negative_score(text):\n",
    "    negword=0\n",
    "    tokenphrase = tokenizer(text)\n",
    "    for word in tokenphrase:\n",
    "        if word in negativeWordList: \n",
    "            negword +=1\n",
    "    retneg = negword \n",
    "    return retneg\n",
    "\n",
    "def polarity_score(positive_score , negative_score):\n",
    "    return (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def AverageSentenceLength (text):\n",
    "    Wordcount = len(tokenizer(text))\n",
    "    SentenceCount = len(sent_tokenize(text))\n",
    "    if SentenceCount > 0: \n",
    "        Average_Sentence_Lenght = Wordcount / SentenceCount\n",
    "    avg = Average_Sentence_Lenght\n",
    "    return round(avg)\n",
    "\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord\n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex\n",
    "\n",
    "def subjectivity(pos,neg,total):\n",
    "    return (pos + neg)/ ((total) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67b7278d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def avg_word_length(content):\n",
    "    return len(content.replace(' ',''))/len(content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23181651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProperNounExtractor(text):\n",
    "    count = 0\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        for (word, tag) in tagged:\n",
    "            if tag == 'PRP': # If the word is a proper noun\n",
    "                count = count + 1 \n",
    "    return(count)\n",
    "\n",
    "\n",
    "def syllable_count(content):\n",
    "    word=content.replace(' ','')\n",
    "    syllable_count=0\n",
    "    for w in word:\n",
    "        if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "    syllable_per_word = syllable_count/len(content.split())\n",
    "    return syllable_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b69dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_words_per_sent(content):\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE=sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE)\n",
    "    return AVG_NUMBER_OF_WORDS_PER_SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdccb4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nthe last 2 years are the most critical time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ntelemedicine, which allows patients and doct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ntelemedicine refers to a specific set of cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nthe future of telehealth stays positive\\nto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nimagine a future where you could see any rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>\\nreconciling with the financial realities of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>\\nwhat is an investment?\\nan investment is a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>\\nquality and affordable healthcare is a visio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>\\nanalytics is a statistical scientific proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>\\nbig data\\nto begin with i shall first like t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    \\nthe last 2 years are the most critical time ...\n",
       "1    \\ntelemedicine, which allows patients and doct...\n",
       "2    \\ntelemedicine refers to a specific set of cli...\n",
       "3    \\nthe future of telehealth stays positive\\nto ...\n",
       "4    \\nimagine a future where you could see any rem...\n",
       "..                                                 ...\n",
       "144  \\nreconciling with the financial realities of ...\n",
       "145  \\nwhat is an investment?\\nan investment is a r...\n",
       "146  \\nquality and affordable healthcare is a visio...\n",
       "147  \\nanalytics is a statistical scientific proces...\n",
       "148  \\nbig data\\nto begin with i shall first like t...\n",
       "\n",
       "[149 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data = [(open(str(i)+'.txt', \"r\", encoding=\"ISO-8859-1\")).read().lower() for i in range(2,151)]\n",
    "  \n",
    "# Create the pandas DataFrame with column name is provided explicitly\n",
    "df = pd.DataFrame(txt_data, columns=['text'])\n",
    "  \n",
    "# print dataframe.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"text\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f760a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AverageSentenceLength\"] = df[\"text\"].apply(AverageSentenceLength)\n",
    "df[\"percentage_complex_word\"] = df[\"text\"].apply(percentage_complex_word)\n",
    "df[\"fog_index\"] = np.vectorize(fog_index)(df['AverageSentenceLength'],df['percentage_complex_word'])\n",
    "df[\"complex_word_count\"] = df[\"text\"].apply(complex_word_count)\n",
    "df[\"total word count\"] = df[\"text\"].apply(total_word_count)\n",
    "df[\"positive_score\"] = df[\"text\"].apply(positive_score)\n",
    "df[\"negative_score\"] = df[\"text\"].apply(negative_score)\n",
    "df[\"polarity_score\"] = np.vectorize(polarity_score)(df['positive_score'],df['negative_score'])\n",
    "df[\"subjectivity\"] = np.vectorize(subjectivity)(df['positive_score'],df['negative_score'], df['total word count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a8a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_length']=df['text'].apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1e6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['personal_pronouns'] = df['text'].apply(ProperNounExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ea83297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['syllable_per_word'] = df['text'].apply(syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02913947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_words_per_sent'] = df['text'].apply(avg_words_per_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f71f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nthe last 2 years are the most critical time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ntelemedicine, which allows patients and doct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ntelemedicine refers to a specific set of cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nthe future of telehealth stays positive\\nto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nimagine a future where you could see any rem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\nthe last 2 years are the most critical time ...\n",
       "1  \\ntelemedicine, which allows patients and doct...\n",
       "2  \\ntelemedicine refers to a specific set of cli...\n",
       "3  \\nthe future of telehealth stays positive\\nto ...\n",
       "4  \\nimagine a future where you could see any rem..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "343f891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = input_df[['URL_ID', 'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53f4f701",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['positive_score', 'negative_score', 'polarity_score', 'subjectivity',\\n       'AverageSentenceLength', 'percentage_complex_word', 'fog_index',\\n       'avg_words_per_sent', 'complex_word_count', 'total word count',\\n       'syllable_per_word', 'personal_pronouns', 'avg_word_length'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extracted_columns \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpositive_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnegative_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolarity_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubjectivity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAverageSentenceLength\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpercentage_complex_word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfog_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_words_per_sent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomplex_word_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal word count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msyllable_per_word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpersonal_pronouns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_word_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mE:\\internship task\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mE:\\internship task\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mE:\\internship task\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['positive_score', 'negative_score', 'polarity_score', 'subjectivity',\\n       'AverageSentenceLength', 'percentage_complex_word', 'fog_index',\\n       'avg_words_per_sent', 'complex_word_count', 'total word count',\\n       'syllable_per_word', 'personal_pronouns', 'avg_word_length'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "extracted_columns = df[['positive_score','negative_score','polarity_score','subjectivity','AverageSentenceLength','percentage_complex_word','fog_index','avg_words_per_sent','complex_word_count','total word count','syllable_per_word','personal_pronouns','avg_word_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "296d2849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-telehealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telemedici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-people-di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/is-telehealth...\n",
       "1       2  https://insights.blackcoffer.com/how-telehealt...\n",
       "2       3  https://insights.blackcoffer.com/is-telemedici...\n",
       "3       4  https://insights.blackcoffer.com/is-telehealth...\n",
       "4       5  https://insights.blackcoffer.com/how-people-di..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819cf85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
